<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title><b>Ming-Yu Liu 劉洺堉</b></title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="style.css" rel="stylesheet" type="text/css" media="screen" />
</head>
<body>
<div id="header">
	<h1><a href="#"><b>Ming-Yu Liu 劉洺堉</b></a></h1>
</div>
</br>
<div id="menu">
	<ul>
        <li class=""><a href="index.html" accesskey="1" title="">Home</a></li>
        <li><a href="publication.html" accesskey="2" title="">Publication</a></li>        
        <li><a href="https://mingyuliu.blog" accesskey="6" title="">Blog</a></li>
	</ul>
</div>
<hr />
<div id="latest-post" class="post">
	<div class="entry"><img src="images/gtc_gaugan.jpg" alt="" width="400" class="left" />		
		<p><h3>Ming-Yu Liu is a distinguished research scientist at NVIDIA Research. Before joining NVIDIA in 2016, he was a principal research scientist at Mitsubishi Electric Research Labs (MERL). He earned his Ph.D. from the Department of Electrical and Computer Engineering at the University of Maryland College Park in 2012. He received the R&D 100 Award by R&D Magazine for his robotic bin picking system in 2014. His semantic image synthesis paper and scene understanding paper are in the best paper finalist in the 2019 CVPR and 2015 RSS conferences, respectively. In SIGGRAPH 2019, he won the Best in Show Award and Audience Choice Award in the Real-Time Live show for his image synthesis work. His research focus is on generative image modeling. His goal is to enable machines human-like imagination capability.</p>
		<p>
			<a href="https://github.com/mingyuliutw" accesskey="6" title=""><img src="images/github.jpg" alt="" width="40" class="left" /></a>
			<a href="https://twitter.com/liu_mingyu" accesskey="6" title=""><img src="images/twitter.jpg" alt="" width="43" class="left" /></a>
			<a href="https://www.linkedin.com/in/mingyuliu/" accesskey="6" title=""><img src="images/linkedin.jpg" alt="" width="42" class="left"/></a>
		</p>
	</div>
	<p class="meta"></p>
</div>

<div id="latest-post" class="post"><h2><b>
	<a href="index.html#research">Research</a>, 
	<a href="index.html#award">Awards</a>, 
    <a href="index.html#service">Research community service</a>, 
    <a href="index.html#tutorial">Tutorials</a>,
    <a href="index.html#workshop">Workshops</a>,
</b></h2>
<hr />

<hr />
<div id= "award">
<div id="latest-post" class="post"><h2><b>Awards</b></h2>
	<div class="entry">
	<ul>
		<li>
		  <h3><p> Best in Show Award and Audience Choice Award, RealTimeLive, SIGGRAPH 2019</h3>		
		</li>			
		<li>
		  <h3><p> Best paper finalist, Computer Vision and Pattern Recognition (CVPR) Conference 2019</h3>		
		</li>			
		<li>
		  <h3><p> 1st place, Domain Adaptation for Semantic Segmentation Competition, WAD Challenge, CVPR 2018</h3>		
		</li>		
		<li>
		  <h3><p> 1st place, Optical Flow Competition, Robust Vision Challenge, CVPR 2018</h3>		
		</li>	
		<li>
		  <h3><p> Outstanding Reviewer, CVPR 2018</h3>		
		</li>
		<li>
		  <h3><p> Pioneer Research Award, NVIDIA 2017, 2018</b></h3>		
		</li>
		<li>
		  <h3><p> NTECH Best Presenter Award, NVIDIA 2017</b></h3>		
		</li>
		<li>
		  <h3><p> CR&D Award, Mitsubishi Electric Research Labs (MERL) 2016</b></h3>		
		</li>			
		<li>
		  <h3><p> Best paper finalist, Robotics Science and Systems (RSS) Conference 2015</a></h3>		
		</li>				  
		<li>		  
		  <h3><p>R&D 100 award, R&D Magazine 2014</a></h3>
		</li>		
		</ul>
	</ul>	
	</div>
</div>	


<div id="research" class="post"><h2><b>Research Focus</b></h2>
	<table style="width:100%">
	<!-- Few-shot-vid2vid -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/few-shot-vid2vid.gif" alt="" width="480" class="left"/>
	    </th>
	    <th style="text-align:left;vertical-align:top">
	    	<h3><a href="https://github.com/NVlabs/few-shot-vid2vid">Few-shot vid2vid</a>:  
			Few-shot Video-to-Video Synthesis (NeurIPS 2019) </br>
			</h3>
			</h3>
		</th> 
    </tr>   
	<!-- FUNIT -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/funit.gif" alt="" width="480" class="left"/>
	    </th>
	    <th style="text-align:left;vertical-align:top">
	    	<h3><a href="https://github.com/NVlabs/FUNIT">FUNIT</a>:  
			Few-shot Unsupervised Image-to-Image Translation (ICCV 2019) </br>
			<a href="https://nvlabs.github.com/FUNIT/petswap.html">Live Demo!</a>
			</h3>
			</h3>
		</th> 
    </tr>    		
	<!-- SPADE -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/spade.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
	    	<h3><a href="https://github.com/NVlabs/SPADE">SPADE</a>:  
			Semantic Image Synthesis with Spatially-Adaptive Normalization (CVPR 2019, Siggraph Real Time Live 2019) </br>
			<a href="https://nvlabs.github.com/SPADE/demo.html">Live Demo!</a>
		</h3>
			</h3>
		</th> 
    </tr>    
    <!-- VID2VID -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/vid2vid.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
	    	<h3><h3><a href="https://github.com/NVIDIA/vid2vid">vid2vid</a>: Video-to-Video Synthesis (NeurIPS 2018)</h3>
			</h3>
		</th> 
    </tr>	
    <!-- MUNIT -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/munit.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
			<h3><a href="https://github.com/NVlabs/MUNIT">MUNIT</a>: Multimodal unsupervised image-to-image translation (ECCV 2018)</h3>
			</h3>
		</th> 
    </tr>
    <!-- FastPhotoStyle -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/fast_photo_style.png" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
			<h3><a href="https://github.com/nvidia/fastphotostyle">FastPhotoStyle</a><br>
			A Closed-form Solution to Photorealistic Image Stylization (ECCV 2018)</h3>
			</h3>
		</th> 
    </tr>
    <!-- pix2pixHD -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/pix2pixHD.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
			<h3><a href="https://github.com/NVIDIA/pix2pixHD">pix2pixHD</a><br>
			High-res image synthesis and semantic manipulation (CVPR 2018)</h3>
			</h3>
		</th> 
    </tr>
    <!-- MoCoGAN -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/mocogan.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
		<h3><a href="https://github.com/sergeytulyakov/mocogan">MoCoGAN</a><br>
		Decomposing Motion and Content for Video Generation (CVPR 2018)</h3>
		</th> 
    </tr>        
    <!-- UNIT -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/unit.gif" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
			<h3><a href="https://github.com/mingyuliutw/UNIT">UNIT</a><br>
			Unsupervised image-to-image translation Network (NeurIPS 2017)</h3>
		</th> 
    </tr>
    <!-- Coupled GAN -->
	<tr>
	    <th>
	    	<div class="entry"><img src="gifs/cogan.png" alt="" width="480" class="left" />
	    </th>
	    <th style="text-align:left;vertical-align:top">
			<h3><a href="https://github.com/mingyuliutw/UNIT">Coupled GAN</a><br>
			Coupled Generative Adversarial Networks (NeurIPS 2016)<br>
			</h3>
		</th> 
    </tr>          
	</table>
</div>

<hr />
<div id="service">
<div id="latest-post" class="post"><h2><b>Research community service</b></h2>
	<div class="entry">
	<ul>
		<li> 
			<h3>Conference program chair: WACV</h3>
		</li>			
		<li> 
			<h3>Conference area chair: ICCV, CVPR, BMVC, WACV</h3>
		</li>		
		<li> 
			<h3>Conference reviewer: CVPR, ICCV, ECCV, NIPS, ICML, ICLR</h3>
		</li>
		<li> 
			<h3>Journal reviewer: TPAMI, IJCV, TIP, TMM, CVIU</h3>
		</li>
		<li> 
			<h3>Journal guess editor: IJCV</h3>
		</li>	
		
		</ul>
	</ul>	
	</div>
</div>	

<hr />
<div id="tutorial">
<div id="latest-post" class="post"><h2><b>Co-hosted Tutorials</b></h2>
	<div class="entry">
	<ul>
		<li>
		  <h3><a href="https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/">[Site]</a> ICCV 2019 Tutorial on Accelerating Computer Vision with Mixed Precision</h3>
		</li>		
		<li>
		  <h3><a href="https://mingyuliutw.github.io/icip2019-image-translation-tutorial/index.html">[Site]</a> ICIP 2019 Tutorial: Image-to-Image Translation</h3>
		</li>
		<li>
		  <h3><a href="https://nvlabs.github.io/dl-for-content-creation/">[Site]</a> CVPR 2019 Tutorial: Deep Learning for Content Creation</h3>
		</li>
		<li>
		  <h3><a href="https://github.com/mingyuliutw/cvpr2017_gan_tutorial">[Site]</a> CVPR 2017 Tutorial: Theory and Applications of Generative Adversarial Networks</h3>
		</li>	
		<li>
		  <h3><a href="tutorial/accv2016_dl_tutorial/index.html">[Site]</a> ACCV 2016 Tutorial: Deep Learning for Vision Guided Language Generation and Image Generation</h3>
		</li>		
		</ul>
	</ul>	
	</div>
</div>	

<hr />
<div id="workshop">
<div id="latest-post" class="post"><h2><b>Co-hosted Workshops</b></h2>
	<div class="entry">
	<ul>
		<li>
		  <h3><a href="https://sites.google.com/berkeley.edu/iccv-2019-image-and-video-syn">[Site]</a> ICCV 2019 Workshop: Image and Video Synthesis: How, Why and "What if"?</h3>
		</li>	
		<li>
		  <h3><a href="http://www.vision.ee.ethz.ch/aim19/">[Site]</a> ICCV 2019 Workshop: Advances in Image Manipulation workshop and challenges on image and video manipulation</h3>
		</li>		
		<li>
		  <h3><a href="http://www.vision.ee.ethz.ch/ntire19/">[Site]</a> CVPR 2019 Workshop: 4th New Trends in Image Restoration and Enhancement workshop and challenges</h3>
		</li>
		<li>
		  <h3><a href="https://www.aicitychallenge.org/">[Site]</a>CVPR 2019 Workshop: AI City Challenge</h3>
		</li>			
		<li>
		  <h3><a href="https://www.aicitychallenge.org/">[Site]</a> CVPR 2018 Workshop: AI City Challenge</h3>
		</li>
		</ul>
	</ul>	
	</div>
</div>	


<div id="footer">
	<p class="legal">&copy;2019 All Rights Reserved.</p>
</div>
</body>
</html>

