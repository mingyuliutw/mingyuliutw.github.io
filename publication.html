<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by Free CSS Templates
http://www.freecsstemplates.org
Released for free under a Creative Commons Attribution 2.5 License

Name       : News Beat
Description: A fixed-width design suitable for small sites and blogs.
Version    : 1.0
Released   : 20071215

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!-- <rule name="Proxy">
  <match url="(.*)" />
  <action type="Rewrite" url="http://mingyuliu.net/{R:1}" />
</rule>	
<META HTTP-EQUIV="Rewrite" CONTENT="3;URL=http://mingyuliu.net"> -->
<!--<META HTTP-EQUIV="refresh" CONTENT="3;URL=https://sites.google.com/site/seanmingyuliu/">-->
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title><b>Ming-Yu Liu 劉洺堉</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="style.css" rel="stylesheet" type="text/css" media="screen" />
</head>
<body>
<div id="header">
	<h1><a href="#"><b>Ming-Yu Liu 劉洺堉</b></a></h1>
</div>
</br>
<div id="menu">
	<ul>
                <li class=""><a href="index.html" accesskey="1" title="">Home</a></li>
                <li><a href="publication.html" accesskey="2" title="">Publication</a></li>
                <li><a href="index.html#awards" accesskey="3" title="">Award</a></li>
                <li><a href="publication.html#service" accesskey="3" title="">Service</a></li>
                <li><a href="publication.html#patent" accesskey="4" title="">Patent</a></li>
                <li><a href="publication.html#tutorial" accesskey="5" title="">Tutorial & Workshop</a></li>
                <li><a href="publication.html#media" accesskey="5" title="">Media Cover</a></li>
                <li><a href="https://mingyuliu.blog" accesskey="6" title="">Blog</a></li>
	</ul>
</div>
<hr />
<div id = "publication">
<div id="latest-post" class="post"><h1><b>Publication</b></h1>
	<div class="entry" id = "publications">

	<ul><h2><b>2019</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1809.06214">Unsupervised Stylish Image Description Generation via Domain Layer Norm</a></b></br>
				Cheng-Kuan Chen, Zhu-Feng Pan, <b>Ming-Yu Liu</b>, Min Sun</br>
				AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019, Honolulu, Hawaii, USA </br>
				arXiv preprint arXiv:1809.06214</br>
				</p>
				</h2>
			</li>		
		</ul>
	</ul>	

	<ul><h2><b>2018</b></h2>
		<ul>			
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1808.06601">Video-to-Video Synthesis</a></b></br>
				Ting-Chun Wang, <b>Ming-Yu Liu</b>, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro</br>
				Neural Information Processing Systems (<b>NIPS</b>) 2018, Montreal, Canada,
			</br>
				arXiv preprint arXiv:1808.06601</br>
				</p>
				<a href="https://github.com/NVIDIA/vid2vid">[Code]</a>
				</h2>
			</li>			
			<li>
				<h2><b><p>Context-aware Synthesis and Placement of Object Instances</b></br>
				Donghoon Lee, Sifei Liu, Jinwei Gu, <b>Ming-Yu Liu</b>, Ming-Hsuan Yang, Jan Kautz</br>
				Neural Information Processing Systems (<b>NIPS</b>) 2018, Montreal, Canada,
			</br>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/pdf/1801.05124.pdf">Localization-Aware Active Learning for Object Detection</a></b></br>
				Chieh-Chi Kao, Teng-Yok Lee, Pradeep Sen, <b>Ming-Yu Liu</b></br>
				Asian Conference on Computer Vision (<b>ACCV</b>) 2018, Perth, Australia,</br>				
				arXiv preprint arXiv:1801.05124</br>
				</p></h2>
			</li>									
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1807.09384">Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation</a></b></br>
				Aysegul Dundar, <b>Ming-Yu Liu</b>, Ting-Chun Wang, John Zedlewski, Jan Kautz</br>
				arXiv preprint arXiv:1807.09384</br>
				</p></h2>
			</li>		
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1804.04732">Multimodal Unsupervised Image-to-Image Translation</a></b></br>
				Xun Huang, <b>Ming-Yu Liu</b>, Serge Belongie, Jan Kautz</br>
				European Conference on Computer Vision (<b>ECCV</b>), 2018, 
				Munich, Germany</br>
				arXiv preprint arXiv:1802.06474</br>
				<a href="https://github.com/NVlabs/MUNIT">[Code]</a><a href="https://youtu.be/ab64TWzWn40">[Video]</a>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="http://arxiv.org/abs/1802.06474">A Closed-form Solution to Photorealistic Image Stylization</a></b></br>
				Yijun Li, <b>Ming-Yu Liu</b>, Xueting Li, Ming-Hsuan Yang, Jan Kautz</br>
				European Conference on Computer Vision (<b>ECCV</b>), 2018, 
				Munich, Germany</br>	
				arXiv preprint arXiv:1802.06474</br>
				<a href="https://github.com/NVIDIA/FastPhotoStyle">[Code]</a>
				<a href="https://www.youtube.com/watch?v=Y4_NPRf3eLM">[Video]</a>
				</p></h2>
			</li>
			<li>
				<h2><b><a href="https://varunjampani.github.io/ssn"><p>Superpixel Sampling Networks</b></a></br>
				Varun Jampani, Deqing Sun, <b>Ming-Yu Liu</b>, Ming-Hsuan Yang, Jan Kautz</br>
				European Conference on Computer Vision (<b>ECCV</b>), 2018, 
				Munich, Germany</br>	
				arXiv preprint arXiv:1802.06474</br>
				</p></h2>
			</li>			
			<li>
				<h2><b><p><a href="https://arxiv.org/pdf/1801.05117.pdf">Reblur2Deblur: Deblurring Videos via Self-Supervised Learning</a></b></br>
				Huaijin Chen, Jinwei Gu, Orazio Gallo, <b>Ming-Yu Liu</b>, Ashok Veeraraghavan, Jan Kautz</br>
				International Conference on Computational Photography (<b>ICCP</b>), 2018, Pittsburgh, Pennsylvania, USA </br>
				arXiv preprint arXiv:1801.05117</br>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1711.11585">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</a></b></br>
				Ting-Chun Wang, <b>Ming-Yu Liu</b>, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Oral 2018, Salt Lake City, Utah</br>
				arXiv preprint arXiv:1711.11585</br>
				<a href="https://tcwang0509.github.io/pix2pixHD/">[Code]</a>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1709.02371">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</a></b></br>
				Deqing Sun, Xiaodong Yang,  <b>Ming-Yu Liu</b>, Jan Kautz</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Oral 2018, Salt Lake City, Utah</br>
				arXiv preprint arXiv:1709.02371</br>
				</p></h2>
			</li>	
			<li>
				<h2><b><p><a href="publication.html">Learning Superpixels with Segmentation-Aware Affinity Loss</a></b></br>
				Wei-Chih Tu, <b>Ming-Yu Liu</b>, Varun Jampani, Deqing Sun, Shao-Yi Chien, Ming-Hsuan Yang, Jan Kautz</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2018, Salt Lake City, Utah</br>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1707.04993">MoCoGAN: Decomposing Motion and Content for Video Generation</a></b></br>
				Sergey Tulyakov, <b>Ming-Yu Liu</b>, Xiaodong Yang, Jan Kautz</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2018, Salt Lake City, Utah</br>
				Neural Information Processing Systems (<b>NIPS</b>) Workshop Track, 2017, Long Beach, California,</br>
				arXiv preprint arXiv:1707.04993</br>
				<a href="https://github.com/sergeytulyakov/mocogan">[Code]</a>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Naphade_The_2018_NVIDIA_CVPR_2018_paper.pdf">The 2018 NVIDIA AI City Challenge</a></b></br>
				Milind Naphade, Ming-Ching Chang, Anuj Sharma, David C. Anastasiu, Vamsi Jagarlamudi, Pranamesh Chakraborty, Tingting Huang, Shuo Wang, <b>Ming-Yu Liu</b>, Rama Chellappa, Jenq-Neng Hwang, and Siwei Lyu</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Workshop 2018, Salt Lake City, Utah</br>
				</p></h2>
			</li>			
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1712.05087">Learning Binary Residual Representations for Domain-specific Video Streaming</a></b></br>
				Yi-Hsuan Tsai, <b>Ming-Yu Liu</b>, Deqing Sun, Ming-Hsuan Yang, Jan Kautz</br>
				AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2018, New Orleans, Louisiana, USA </br>
				<a href="https://mingyuliu.blog/2018/01/30/binary-residual-autoencoder/">[Blog]</a>
				<a href="https://youtu.be/SCc0SqzkQf4">[Video]</a>
				</p></h2>
			</li>	
		</ul>
	</ul>			
	<ul><h2><b>2017</b></h2>
		<ul>		
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1703.00848">Unsupervised Image-to-Image Translation Networks</a></b></br>
				<b>Ming-Yu Liu</b>, Thomas Breuel, Jan Kautz</br>
				Neural Information Processing Systems (<b>NIPS</b>) Spotlight, 2017, Long Beach, California,</br>
				arXiv preprint arXiv:1703.00848</br>
				<a href="https://github.com/mingyuliutw/UNIT">[Code]</a>
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1710.00814">Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight</a></b></br>
				Yen-Chen Lin, <b>Ming-Yu Liu</b>, Min Sun, Jia-Bin Huang</br>
				Neural Information Processing Systems (<b>NIPS</b>) Workshop Track, 2017, Long Beach, California,</br>
				arXiv preprint arXiv:1710.00814</br>
				<a href="https://github.com/yenchenlin/rl-attack-detection">[Code]</a>
				</p></h2>
			</li>	
      		<li>
				<h2><b><p><a href="https://arxiv.org/abs/1703.06748">Tactics of Adversarial Attack on Deep Reinforcement Learning Agents</a></b></br>
				Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, <b>Ming-Yu Liu</b>, Min Sun </br>
				International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2017 Melbourne, Australia </br>
				International Conference on Learning Representation (<b>ICLR</b>) Workshop Track, 2017 Toulon, France </br>
				arXiv preprint arXiv:1703.06748</br>
				<a href="http://yclin.me/adversarial_attack_RL">[Project]</a>
				</p></h2>
			</li>				
      		<li>
				<h2><b><p><a href="https://arxiv.org/abs/1705.01759">Deep 360 Pilot: Learning a Deep Agent for Piloting through 360 Sports Videos</a></b></br>
				Hou-Ning Hu*, Yen-Chen Lin*, <b>Ming-Yu Liu</b>, Hsien-Tzu Cheng, Stanley Chang, Min Sun </br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Oral, 2017, Honolulu, Hawaii</br>
				arXiv preprint arXiv:1705.01759</br>
				<a href="https://github.com/eborboihuc/Deep360Pilot-CVPR17">[Code]</a>
				</p></h2>
			</li>			
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1705.09759">CASENet: Deep Category-Aware Semantic Edge Detection</a></b></br>
				Zhiding Yu, Chen Feng, <b>Ming-Yu Liu</b>, Srikumar Ramalingam</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2017, Honolulu, Hawaii</br>
				arXiv preprint arXiv:1705.09759</br>
				<a href="http://www.merl.com/research/?research=license-request&sw=CASENet">[Code]</a>
				</p></h2>
			</li>  			
			<li>
				<h2><b><p><a href="https://www.merl.com/publications/docs/TR2017-034.pdf">Deep Active Learning for Civil Infrastructure Defect Detection and Classification</a></b></br>
				Chen Feng, <b>Ming-Yu Liu</b>, Chieh-Chi Kao, Teng-Yok Lee</br>
				International Workshop on Computing in Civil Engineering (<b>IWCCE</b>), 2017, Seatle, Washington
				</p></h2>
			</li>
			<li>
				<h2><b><p><a href="https://arxiv.org/abs/1702.01478">Attentional Network for Visual Object Detection</a></b></br>
				Kota Hara, <b>Ming-Yu Liu</b>,  Oncel Tuzel, Amir-massoud Farahmand</br>
				arXiv preprint arXiv:1702.01478</br>
				</p></h2>
			</li>			
		</ul>
	</ul>		
	<ul><h2><b>2016</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://arxiv.org/abs/1606.07536">Coupled Generative Adversarial Networks</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel</br>
				Neural Information Processing Systems (<b>NIPS</b>), 2016, Barcelona, Spain,</br>
				arXiv preprint arXiv:1606.07536</br>
				<a href="https://github.com/mingyuliutw/CoGAN">[Code]</a></p></h2>
			</li>		
		</ul>		
		<ul>
			<li>
				<h2><b><p><a href="https://merl.com/publications/docs/TR2016-144.pdf">R-CNN for Small Object Detection</a></b></br>
				Chenyi Chen, <b>Ming-Yu Liu</b>, Oncel Tuzel, Jianxiong Xiao</br>
				Asian Conference on Computer Vision  (<b>ACCV</b>), 2016, Taipei, Taiwan
				</p></h2>
			</li>		
		</ul>				
		<ul>
			<li>
				<h2><b><p><a href="http://ravitejav.weebly.com/uploads/2/4/7/2/24725306/segmentation.pdf">Gaussian Conditional Random Field Network for Semantic Segmentation</a></b></br>
				Raviteja Vemulapalli, Oncel Tuzel, <b>Ming-Yu Liu</b>, Rama Chellappa</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) Spotlight, 2016, Las Vegas, Nevada, </br>
				</p></h2>
			</li>		
		</ul>		
		<ul>
			<li>
				<h2><b><p><a href="http://arxiv.org/pdf/1511.04067.pdf">Deep Gaussian Conditional Random Field Network: A Model-based Deep Network for Discriminative Denoising</a></b></br>
				Raviteja Vemulapalli, Oncel Tuzel, <b>Ming-Yu Liu</b></br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2016, Las Vegas, Nevada, </br>
				arXiv preprint arXiv:1511.04067</br>
				</p></h2>
			</li>		
		</ul>	

		<ul>
			<li>
				<h2><b><p><a href="http://arxiv.org/abs/1601.01750">Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup</a></b></br>
				Kilho Son, <b>Ming-Yu Liu</b>, Yuichi Taguchi</br>
				IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2016, Stockholm, Sweden</br>
				arXiv preprint arXiv:1601.01750</br>
				<a href="https://github.com/kilho/learning_tof">[Dataset]</a>
				</p></h2>
			</li>		
		</ul>	
		<ul>
			<li>
				<h2><b><p><a href="http://arxiv.org/abs/1502.05689">Unsupervised Network Pretraining via Encoding Human Design</a></b></br>
				<b>Ming-Yu Liu</b>, Arun Mallya, Oncel Tuzel, Xi Chen</br>
				IEEE Winter Conference on Computer Vision (<b>WACV</b>), 2016, New York, USA</br>
				arXiv preprint arXiv:1502.05689				
				</p></h2>
			</li>		
		</ul>
	</ul>	
	<ul><h2><b>2015</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://www.roboticsproceedings.org/rss11/p25.html">Layered Interpretation of Street View Images</a></b></br>
				<b>Ming-Yu Liu</b>, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel</br>
				Robotics: Science and Systems Conference (<b>RSS</b>) Best paper finalist, 2015, Rome, Italy</br>
				arXiv preprint arXiv:1506.04723
				</p></h2>
			</li>		
		</ul>
	</ul>	
	<ul><h2><b>2014</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://papers.nips.cc/paper/5282-recursive-context-propagation-network-for-semantic-scene-labeling.pdf">Recursive Context Propagation Network for Semantic Scene Labeling</a></b></br>
				Abhishek Sharma, Oncel Tuzel, <b>Ming-Yu Liu</b></br>
				Neural Information Processing Systems (<b>NIPS</b>), 2014, Montreal, Canada
				</p></h2>
			</li>	
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2014-078.pdf">Learning to Rankd 3D Features</a></b></br>
				Oncel Tuzel, <b>Ming-Yu Liu</b>, Yuichi Taguchi, Arvind Raghunathan</br>
				European Conference on Computer Vision (<b>ECCV</b>), 2014, Zurich, Switzerland 
				</p></h2>
			</li>				
		</ul>
	</ul>
	<ul><h2><b>2013</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2013-042.pdf">Joint Geodesic Upsampling of Depth Images</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Yuichi Taguchi</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2013, Portland, Oregon, USA</br><a href="https://github.com/mingyuliutw/jgu.git">[Results]</a><a href="http://www.merl.com/research/?research=license-request&sw=JGU">[Code]</a>
				</p></h2>
			</li>	
			<li>
				<h2><b><p><a href="papers/liu_entropy_tpami14.pdf">Entropy Rate Clustering: Cluster Analysis via Maximizing a Submodular Function Subject to a Matroid Constraint</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Srikumar Ramalingam, Rama Chellappa</br>
				IEEE Transaction on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2013 </br><a href="papers/liu_entropy_proofs_tpami14.pdf">[Proofs]</a>
				<a href="https://github.com/mingyuliutw/ers.git">[Code]</a></p></h2>
			</li>				
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2013-055.pdf">Model-based Vehicle Pose Estimation and Tracking in Videos Using Random Forests</a></b></br>
				Michael Hodlmoser, Branislav Micusik, Marc Pollefeys, <b>Ming-Yu Liu</b>, Martin Kampel</br>
				International Conference on 3D Vision (<b>3DV</b>) 2013, Seattle, Washington, USA
				</p></h2>
			</li>		
		</ul>
	</ul>
	<ul><h2><b>2012</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2012-007.pdf">Fast Object Localization and Pose Estimation in Heavy Clutter for Robotic Bin Picking</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Ashok Veeraraghavan, Yuichi Taguchi, Tim K. Marks, Rama Chellappa</br>
				International Journal of Robotics Research (<b>IJRR</b>) 2012
				</br><a href="https://github.com/mingyuliutw/fdcm.git">[Code]</a></p></h2>
			</li>	
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2012-032.pdf">Voting-Based Pose Estimation for Robotic Assembly Using a 3D Sensor</a></b></br>
				Changhyun Choi, Yuichi Taguchi, Oncel Tuzel, <b>Ming-Yu Liu</b>, Srikumar Ramalingam</br>
				International Conference on Robotics and Automation (<b>ICRA</b>), 2012, St. Paul, Minnesota, USA
				</p></h2>
			</li>				
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2012-092.pdf">A Grassmann Manifold-based Domain Adaptation Approach</a></b></br>
				Jingjing Zheng, <b>Ming-Yu Liu</b>, Rama Chellappa, P Jonathan Phillips</br>
				International Conference on Pattern Recognition (<b>ICPR</b>) Oral, 2012, Tsukuba Science City, Japan
				</p></h2>
			</li>		
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2012-081.pdf">Classification and Pose Estimation of Vehicles in Videos by 3D Modeling within Discrete-Continuous Optimization</a></b></br>
				Michael Hödlmoser, Branislav Micusik, <b>Ming-Yu Liu</b>, Marc Pollefeys, Martin Kampel</br>
				International Conference on 3D Vision (<b>3DV</b>) 2012, Zurich, Switzerland
				</p></h2>
			</li>			
		</ul>
	</ul>
	<ul><h2><b>2011</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2011-035.pdf">Entropy Rate Superpixel Segmentation</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Srikumar Ramalingam, Rama Chellappa</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2011, Colorado Springs, Colorado, USA</br><a href="https://github.com/mingyuliutw/ers.git">[Code]</a>
				</p></h2>
			</li>			
		</ul>
	</ul>		
	<ul><h2><b>2010</b></h2>
		<ul>
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2010-045.pdf">Fast Directional Chamfer Matching</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Ashok Veeraraghavan, Rama Chellappa</br>
				Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2010, San Francisco, California, USA </br><a href="https://github.com/mingyuliutw/fdcm.git">[Code]</a>
				</p></h2>
			</li>			
			<li>
				<h2><b><p><a href="http://www.merl.com/publications/docs/TR2010-024.pdf">Pose Estimation in Heavy Clutter Using a Multi-Flash Camera</a></b></br>
				<b>Ming-Yu Liu</b>, Oncel Tuzel, Ashok Veeraraghavan, Rama Chellappa, Amit Agrawal, Haruhisa Okuda</br>
				International Conference on Robotics and Automation (<b>ICRA</b>), 2010, Anchorage, Alaska, USA </br><a href="https://github.com/mingyuliutw/fdcm.git">[Code]</a>
				</p></h2>
			</li>					
		</ul>
	</ul>		
	</div>
</div>	
</div>

<hr />
<div id="patent">
<div id="latest-post" class="post"><h1><b>Patent</b></h1>
	<div class="entry">
	<ul>
		<li><h2><p> US 9,989,964: System and Method for Controlling Vehicle Using Neural Network</p></h2></li>
		<li><h2><p> US 9,971,958: Method and System for Generating Multimodal Digital Images</p></h2></li>
		<li><h2><p> US 9,811,756: Method for Labeling Images of Street Scenes</p></h2></li>
		<li><h2><p> US 9,805,294: Method for Denoising Time-of-Flight Range Images</p></h2></li>
		<li><h2><p> US 9,704,257: System and method for semantic segmentation using Gaussian random field network</p></h2></li>
		<li><h2><p> US 9,633,274: Method and system for denoising images using deep Gaussian conditional random field network</p></h2></li>
		<li><h2><p> US 9,558,268: Method for semantically labeling an image of a scene using recursive context propagation</p></h2></li>
		<li><h2><p> US 9,280,827: Method for determining object poses using Weighted Features</p></h2></li>	
		<li><h2><p> US 9,195,904: Method for detecting objects in stereo images</p></h2></li>		
		<li><h2><p> US 8,983,177: Method for increasing resolutions of depth images</p></h2></li>
		<li><h2><p> US 8,908,913: Voting-based pose estimation for 3D sensors</p></h2></li>
		<li><h2><p> US 8,428,363: Method for segmenting images using superpixels and entropy rate clustering</p></h2></li>
		<li><h2><p><a href="https://patents.google.com/?assignee=Mitsubishi+Electric&inventor=Ming-Yu+Liu">My MERL patents</a></p></h2></li>
	</ul>	
	</div>
</div>

<hr />
<div id="talk">
<div id="latest-post" class="post"><h1><b>Invited Talk</b></h1>
	<div class="entry">
	<ul>
		<li>
		  <h2><b>Video-to-Video Synthesis,</br>
		  <a href="http://chalearnlap.cvc.uab.es/workshop/29/program/">Chalearn Looking at People Satellite Workshop ECCV</a>, ECCV 2018 Workshop</br>
		  <a href="https://drive.google.com/open?id=1_81MOM-RIuXfzu1zDYMOlZtl-IYTzYu6">[Slides]</a></h2>
		</h2>
		</li>	
		<li>
		  <h2><b>Multimodal Image Domain Transfer,</br>
		  <a href="https://sites.google.com/view/task-cv2018/home">TRANSFERRING AND ADAPTING SOURCE KNOWLEDGE IN COMPUTER VISION AND VISDA CHALLENGE</a>, ECCV 2018 Workshop</br></h2>
		  <a href="https://drive.google.com/open?id=13nNm9S43HHrzTSZ-Xeqj-G0Im8DdWXN6">[Slides]</a></h2>
		</li>	
		<li>
		  <h2><b>High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,</br>
		  <a href="http://www.vision.ee.ethz.ch/ntire18/">New Trends in Image Restoration and Enhancement workshop and challenges on super-resolution, dehazing, and spectral reconstruction</a>, CVPR 2018 Workshop</br>
		  <a href="http://www.vision.ee.ethz.ch/ntire18/talks/Ming-YuLiu_pix2pixHD_NTIRE2018talk.pdf">[Slides]</a></h2>
		</li>	
		<li>
		  <h2><b>Multimodal Unsupervised Image-to-Image Translation,</br>
		  <a href="https://sites.google.com/view/cvpr2018tutorialongans/">Tutorial on GANs</a>, CVPR 2018 Tutorial</br>
		  <a href="http://efrosgans.eecs.berkeley.edu/CVPR18_slides/MUNIT_by_Liu.pdf">[Slides]</a></h2>
		</li>
	</ul>	
	</div>
</div>	


<hr />
<div id="tutorial">
<div id="latest-post" class="post"><h1><b>Tutorial and Workshop</b></h1>
	<div class="entry">
	<ul>
		<li>
		  <h2>CVPR 2019 Workshop: 4th New Trends in Image Restoration and Enhancement workshop and challenges</h2>
		</li>
		<li>
		  <h2>CVPR 2019 Workshop: AI City Challenge, <a href="https://www.aicitychallenge.org/">[Site]</a></h2>
		</li>			
		<li>
		  <h2>ICIP 2019 Tutorial: Image-to-Image Translation</h2>
		</li>	
		<li>
		  <h2>CVPR 2018 Workshop: AI City Challenge, <a href="https://www.aicitychallenge.org/">[Site]</a></h2>
		</li>	
		<li>
		  <h2>CVPR 2017 Tutorial: Theory and Applications of Generative Adversarial Networks, <a href="https://github.com/mingyuliutw/cvpr2017_gan_tutorial">[Site]</a></h2>
		</li>	
		<li>
		  <h2>ACCV 2016 Tutorial: Deep Learning for Vision Guided Language Generation and Image Generation, <a href="tutorial/accv2016_dl_tutorial/index.html">[Site]</a></h2>
		</li>		
		</ul>
	</ul>	
	</div>
</div>	


<hr />
<div id="media">
<div id="latest-post" class="post"><h1><b>Media Coverage</b></h1>
	<div class="entry">
	<ul>
		<li><a href="https://wccftech.com/nvidia-creates-interactive-world-ai/">WccfTech: "NVIDIA Creates Interactive World with Its Deep Learning-Based AI Model: ‘It Wouldn’t Have Been Possible Before Tensor Cores’"</a>
		</li>

		<li><a href="https://www.ubergizmo.com/2018/12/nvidia-ai-transform-videos-into-virtual/">Ubergizmo: "NVIDIA’s AI Can Generate Virtual Landscapes Using Real-World Videos"</a>
		</li>

		<li><a href="https://www.theverge.com/2018/12/3/18121198/ai-generated-video-game-graphics-nvidia-driving-demo-neurips">The Verge: "Nvidia has created the first video game demo using AI-generated graphics"</a>
		</li>

		<li><a href="https://venturebeat.com/2018/12/03/how-nvidia-researchers-are-making-virtual-worlds-with-real-world-videos/">VentureBeat: "How Nvidia researchers are making virtual worlds with real-world videos"</a>
		</li>

		<li><a href="https://www.tomshardware.com/news/nvidia-ai-research-render-graphics,38185.html">Tom’s Hardware: "Nvidia Uses Artificial Intelligence to Render Virtual Worlds in Real Time"</a>
		</li>

		<li><a href="https://www.techspot.com/news/77681-nvidia-showcases-ai-rendered-interactive-virtual-world.html">TechSpot: "Nvidia showcases an AI-rendered, interactive virtual world"</a>
		</li>

		<li><a href="https://sdtimes.com/ai/sd-times-news-digest-nvidias-new-ai-research-androids-sdk-initiative-and-thundra-layers-for-aws-lambda/">SD Times news digest: "NVIDIA’s new AI research, Android’s SDK initiative, and Thundra Layers for AWS Lambda"</a>
		</li>

		<li><a href="https://www.theregister.co.uk/2018/12/04/nvidia_neural_network/">The Register: "Like the Architect in the Matrix... but not: Nvidia trains neural network to generate cityscapes"</a>
		</li>

		<li><a href="https://www.pcper.com/news/General-Tech/NVIDIA-Introduces-AI-Interactive-Graphics-Research-3D-Real-World-Video">PC Perspective: "NVIDIA Introduces AI Interactive Graphics Research: 3D from Real-World Video"</a>
		</li>

		<li><a href="https://www.pcmag.com/news/365235/nvidia-taps-ai-to-build-game-environments-from-real-footage">PC Magazine: "Nvidia Taps AI to Build Game Environments From Real Footage"</a>
		</li>
		<li><a href="AI software can dream up an entire digital world from a simple sketch">MIT Technology Review: "NVIDIA’s New AI Builds Interactive Environments From Real Video"</a>
		</li>
		<li><a href="https://www.gamerevolution.com/news/464849-nvidia-new-ai-builds-interactive-environments-real-video">Game Revolution: "NVIDIA’s New AI Builds Interactive Environments From Real Video"</a>
		</li>
		<li><a href="https://futurism.com/ai-dreams-trippy-video-games-based-real-life">Futurism: "New AI Dreams Up Trippy Video Games Based on Real Life Video"</a>
		</li>
		<li><a href="https://www.extremetech.com/extreme/281649-nvidia-ai-can-render-complete-urban-environments-in-unreal-engine-4">ExtremeTech: "Nvidia AI Can Render Complete Urban Environments in Unreal Engine 4"</a>
		</li>
		<li><a href="https://www.engadget.com/2018/12/03/nvidia-ai-video-to-video-synthesis/">Engadget: "NVIDIA's new AI turns videos of the real world into virtual landscapes"</a>
		</li>
		<li><a href="https://www.eetimes.com/document.asp?doc_id=1334030">EE Times: "Nvidia: Forget Graphics, AI Uses Video for 3D"</a>
		</li>
		<li><a href="https://www.digitaltrends.com/computing/nvidia-researchers-use-artificial-intelligence-to-upgrade-your-games-graphics/">Digital Trends: "Nvidia researchers use artificial intelligence to upgrade your game’s graphics"</a>
		</li>
		<li><a href="https://babeltechreviews.com/nvidia-introduces-ai-interactive-graphics/">BabelTechReviews: "NVIDIA Introduces AI Interactive Graphics"</a>
		</li>
		<li><a href="https://www.alphr.com/artificial-intelligence/1010278/nvidia-s-ai-creates-game-demo-entirely-on-its-own">Alphr: "Nvidia’s AI Creates Game Demo Entirely on its Own"</a>
		</li>
		<li><a href="http://in.pcmag.com/photography/119926/news/nvidias-new-image-style-transfer-algorithm-could-be-a-game-c">PC Magazine: "Nvidia's new image style transfer algorithm could be a game changer"</a>
		</li>
		<li><a href="http://www.ubergizmo.com/2018/03/nvidia-ai-apply-photo-style-to-another/">Ubergizmo: "NVIDIA’s AI Can Apply One Photo’s Style To Another"</a>
		</li>
		<li><a href="https://petapixel.com/2018/03/21/nvidias-ai-can-magically-transfer-one-photos-style-to-another/">PetaPixel: "NVIDIA’s AI Can Magically Transfer One Photo’s Style to Another"</a>
		</li>			
		<li><a href="https://www.nytimes.com/interactive/2018/01/02/technology/ai-generated-photos.html">The New York Times: "How an A.I. ‘Cat-and-Mouse Game’
Generates Believable Fake Photos"</a>
		</li>			
		<li><a href="https://www.theverge.com/2017/12/5/16737260/ai-image-translation-nvidia-data-self-driving-cars">The Verge: "Nvidia uses AI to make it snow on streets that are always sunny."</a>
		</li>
		<li><a href="https://www.technologyreview.com/the-download/609667/nvidias-tag-teaming-ais-imagine-night-as-day-and-house-cats-as-tigers/">MIT Technology Review: "Nvidia’s Tag-Teaming AIs Imagine Night as Day, and House Cats as Tigers"</a>
		</li>
		<li><a href="https://www.digitaltrends.com/cool-tech/nvidia-ai-winter-summer-car/">Digital Trend: "Watch Nvidia’s powerful A.I. change day into night, and winter into summer"</a>
		</li>
		<li><a href="https://thenextweb.com/artificial-intelligence/2017/12/04/nvidias-new-ai-creates-disturbingly-convincing-fake-videos/">The Next Web: "Nvidia’s new AI creates disturbingly convincing fake videos"</a>
		</li>
		<li><a href="http://www.zdnet.com/article/nvidia-looks-to-reduce-ai-training-material-through-imagination/#ftag=RSSbaffb68">ZDNet "Nvidia looks to reduce AI training material through 'imagination'"</a>
		</li>
		<li><a href="http://resourcemagonline.com/2017/12/prospective-a-i-software-will-be-able-to-change-the-weather-time-and-season-of-your-photos/83471/">RESOURCE "PROSPECTIVE A.I. SOFTWARE WILL BE ABLE TO CHANGE THE WEATHER, TIME, AND SEASON OF YOUR PHOTOS"</a>
		<li><a href="http://www.slate.com/articles/technology/future_tense/2017/12/a_i_neural_photo_and_image_style_transfer_will_change_the_art_world.html">SLATE "These Stunning A.I. Tools Are About to Change the Art World"</a>
		</li>
		<li><a href="https://futurism.com/ai-makes-fake-videos-facilitate-end-reality-know-it/">Futurism "An AI That Makes Fake Videos May Facilitate the End of Reality as We Know It"</a>
		</li>
		<li><a href="https://www.redsharknews.com/technology/item/5191-nvidia-turns-summer-into-winter-with-ai-video-manipulation">RedShark: "Nvidia turns winter into summer with AI video manipulation"</a>
		</li>	
		<li><a href="https://www.techrepublic.com/article/training-ai-models-is-getting-faster-and-taking-less-data-thanks-to-nvidia-researchers/">TechRepublic "Training AI models is getting faster and taking less data, thanks to NVIDIA researchers"</a>
		</li>
		<li><a href="https://motherboard.vice.com/en_us/article/xwvz9a/watch-an-algorithm-turn-winter-into-summer-in-any-video-image-to-image-translation">Motherboard "Nvidia researchers developed a way to turn snowy roads into summer, and day into night."</a>
		</li>
		<li><a href="http://it.sohu.com/20171006/n516267266.shtml">Sohu technology news "超级变变变：喵星人汪星人还有街景神奇变身"</a>
		</li>	
		<li><a href="http://www.sohu.com/a/208157474_473283">Sohu technology news "条件GAN高分辨率图像合成与语义编辑pix2pixHD"</a>
		</li>
		<li><a href="https://www.inverse.com/article/39018-these-neutral-networks-can-generate-realistic-photos">Inverse.com: "This Photo Is a Total Fake Made by A.I. Its Creators Tell Us How It's Done"</a>
		</li>	
		</li>
		<li><a href="https://www.analyticsvidhya.com/blog/2018/04/framework-for-unsupervised-image-to-image-translation">Analytics Vidhya: "Here’s a Deep Learning Algorithm that Transforms an Image into a Completely Different Category"</a>
		<li><a href="https://www.wired.com/story/will-deepfakes-disrupt-the-midterm-election/">Wired: "Will `Deepfakes` disrupt the midterm election"</a>
		</li>			
		</li>		
					
	</ul>
	</div>
</div>

<div id="footer">
	<p class="legal">&copy;2019 All Rights Reserved.</p>
</div>
</body>
</html>

